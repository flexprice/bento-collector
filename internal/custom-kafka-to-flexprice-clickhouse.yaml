# ============================================================================
# Kafka to ClickHouse Pipeline - Raw Events
# ============================================================================
# Flow: Kafka → Transform → Batch → ClickHouse
# 1. Consume events from Kafka topic
# 2. Transform and normalize event data
# 3. Batch events (1000 per batch or 1s timeout)
# 4. Insert into ClickHouse using native protocol

input:
  kafka:
    addresses:
      - ${FLEXPRICE_KAFKA_BROKERS}

    topics:
      - ${FLEXPRICE_KAFKA_TOPIC:bento-testing}

    consumer_group: ${FLEXPRICE_KAFKA_CONSUMER_GROUP:bento-raw-clickhouse-v1}
    start_from_oldest: true

    tls:
      enabled: true

    sasl:
      mechanism: ${FLEXPRICE_KAFKA_SASL_MECHANISM:PLAIN}
      user: ${FLEXPRICE_KAFKA_SASL_USER}
      password: ${FLEXPRICE_KAFKA_SASL_PASSWORD}

    client_id: ${FLEXPRICE_KAFKA_CLIENT_ID:bento-raw-clickhouse}

    commit_period: 1s
    max_processing_period: 100ms
    fetch_buffer_cap: ${KAFKA_FETCH_BUFFER_CAP:20}
    checkpoint_limit: ${KAFKA_CHECKPOINT_LIMIT:20}

pipeline:
  processors:
    # ============================================================================
    # STEP 1: Transform raw event into ClickHouse schema
    # ============================================================================
    # - Extract core fields (id, customer, timestamp)
    # - Build event_name from provider/service/method/model
    # - Map additional fields to flexible analytics columns
    # - All fields are null-safe with .or("") fallbacks
    # ============================================================================
    # ---- Normalize + build raw payload ----
    - mapping: |
        root = {}

        # Generate UUID if id is missing or null
        root.id = this.id.string().or(uuid_v4())
        root.tenant_id = env("FLEXPRICE_TENANT_ID").string().or("")
        root.environment_id = env("FLEXPRICE_ENVIRONMENT_ID").string().or("")
        root.external_customer_id = this.orgId.string().or("")
        root.source = this.targetItemId.string().or("")
        
        # Timestamp: parse ISO 8601 with timezone, then format for ClickHouse
        # 1. Check if createdAt exists and is valid
        # 2. Parse it as timestamp to preserve timezone info
        # 3. Format to ClickHouse DateTime64 format: "2006-01-02 15:04:05.999999"
        root.timestamp = if this.createdAt != null && this.createdAt.string().or("") != "" && this.createdAt.string() != "null" {
          this.createdAt.ts_parse("2006-01-02T15:04:05.999Z07:00").ts_format("2006-01-02 15:04:05.999999")
        } else {
          timestamp_unix().ts_format("2006-01-02 15:04:05.999999")
        }

        # ---- Build event_name SAFELY ----
        root.event_name = ""

        if this.providerName.string().or("") != "" {
          root.event_name = this.providerName.string().trim().lowercase()
        } else if this.serviceName.string().or("") != "" {
          root.event_name = this.serviceName.string().trim().lowercase()
        }

        if this.methodName.string().or("") != "" {
          if this.methodName.string() == "BEDROCK_LLM" {
            root.event_name = root.event_name + "-modeling"
          } else {
            root.event_name = root.event_name + "-" + this.methodName.string().trim().lowercase()
          }
        }

        if this.data != null && this.data.modelName.string().or("") != "" {
          root.event_name = root.event_name + "-" + this.data.modelName.string().trim().lowercase()
        }

        # ---- Raw payload ----
        root.payload = this.string()

        # ---- Flexible analytics fields ----
        root.field1 = this.providerName.string().or("")
        root.field2 = if this.data != null { this.data.modelName.string().or("") } else { "" }
        root.field3 = this.methodName.string().or("")
        root.field4 = this.byok.string().or("")
        root.field5 = this.dataInterface.string().or("")
        root.field6 = this.referenceType.string().or("")
        root.field7 = this.targetItemType.string().or("")
        root.field8 = this.referenceCost.string().or("")

# ============================================================================
# STEP 2: Batch and Insert into ClickHouse
# ============================================================================
# - Batch up to 1000 events or flush after 1 second
# - Use native ClickHouse protocol (port 9000) for better performance
# - 8 parallel connections for high throughput
# - Auto-retry on failures
# ============================================================================
output:
  sql_insert:
    driver: clickhouse
    
    # Use native ClickHouse protocol (port 9000) - more efficient than HTTP
    # DSN is provided as a complete connection string from environment variable
    dsn: "${FLEXPRICE_CLICKHOUSE_DSN}"
    
    table: flexprice.raw_events
    
    columns:
      - id
      - tenant_id
      - environment_id
      - external_customer_id
      - event_name
      - source
      - payload
      - field1
      - field2
      - field3
      - field4
      - field5
      - field6
      - field7
      - field8
      - timestamp
    
    args_mapping: |
      root = [
        this.id,
        this.tenant_id,
        this.environment_id,
        this.external_customer_id,
        this.event_name,
        this.source,
        this.payload,
        this.field1,
        this.field2,
        this.field3,
        this.field4,
        this.field5,
        this.field6,
        this.field7,
        this.field8,
        this.timestamp
      ]
    
    batching:
      count: ${CLICKHOUSE_BATCH_COUNT:1000}        # 500–1000 batch size (use 500 if traffic is low)
      period: ${CLICKHOUSE_BATCH_PERIOD:1s}         # flush every second even if batch not full
      jitter: 0.1        # avoids thundering herd
    
    max_in_flight: ${CLICKHOUSE_MAX_IN_FLIGHT:8}     # 8 parallel async inserts to ClickHouse

logger:
  level: ${LOG_LEVEL:INFO}
  format: logfmt
  add_timestamp: true
