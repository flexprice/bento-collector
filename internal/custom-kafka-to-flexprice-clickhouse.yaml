# ============================================================================
# Kafka to ClickHouse Pipeline - Raw Events (ENHANCED)
# ============================================================================
# Flow: Kafka → Transform → Batch → ClickHouse + Kafka with tenant/env metadata
# 1. Consume events from Kafka topic
# 2. Transform and normalize event data
# 3. Batch events (1000 per batch or 1s timeout)
# 4. Insert into ClickHouse using native protocol
# 5. Send to Kafka with tenant/environment metadata

input:
  kafka:
    addresses:
      - ${FLEXPRICE_KAFKA_BROKERS}

    topics:
      - ${FLEXPRICE_KAFKA_TOPIC:bento-testing}

    consumer_group: ${FLEXPRICE_KAFKA_CONSUMER_GROUP:bento-raw-clickhouse-v1}
    start_from_oldest: true

    tls:
      enabled: true

    sasl:
      mechanism: ${FLEXPRICE_KAFKA_SASL_MECHANISM:PLAIN}
      user: ${FLEXPRICE_KAFKA_SASL_USER}
      password: ${FLEXPRICE_KAFKA_SASL_PASSWORD}

    client_id: ${FLEXPRICE_KAFKA_CLIENT_ID:bento-raw-clickhouse}

    # Commit period: how often to commit offsets to Kafka
    commit_period: ${KAFKA_COMMIT_PERIOD:10s}
    
    # Max processing period: max time messages can be pending before forced flush
    max_processing_period: ${KAFKA_MAX_PROCESSING_PERIOD:120s}
    
    # Fetch buffer: number of messages to buffer from Kafka (larger = more throughput)
    fetch_buffer_cap: ${KAFKA_FETCH_BUFFER_CAP:10000}
    
    # Checkpoint limit: CRITICAL FOR ZERO DATA LOSS
    # Number of messages to process before committing offset
    # Set to 1000 to ensure offsets committed only after successful ClickHouse writes
    checkpoint_limit: ${KAFKA_CHECKPOINT_LIMIT:1000}

pipeline:
  processors:
    # ============================================================================
    # STEP 0: Store original raw event for Kafka output
    # ============================================================================
    - mapping: |
        # Store the original raw event in metadata for later use
        meta original_raw_event = this.string()
        
        # Store tenant and environment IDs in metadata for Kafka output
        meta tenant_id = env("FLEXPRICE_TENANT_ID").string().or("")
        meta environment_id = env("FLEXPRICE_ENVIRONMENT_ID").string().or("")

    # ============================================================================
    # STEP 1: Transform raw event into ClickHouse schema
    # ============================================================================
    # - Extract core fields (id, customer, timestamp)
    # - Build event_name from provider/service/method/model
    # - Map additional fields to flexible analytics columns
    # - All fields are null-safe with .or("") fallbacks
    # ============================================================================
    # ---- Normalize + build raw payload ----
    - mapping: |
        root = {}

        # Generate UUID if id is missing or null
        root.id = this.id.string().or(uuid_v4())
        root.tenant_id = env("FLEXPRICE_TENANT_ID").string().or("")
        root.environment_id = env("FLEXPRICE_ENVIRONMENT_ID").string().or("")
        root.external_customer_id = this.orgId.string().or("")
        root.source = this.targetItemId.string().or("")
        
        # Timestamp: parse ISO 8601 with timezone, then format for ClickHouse
        # 1. Check if createdAt exists and is valid
        # 2. Parse it as timestamp to preserve timezone info
        # 3. Format to ClickHouse DateTime64 format: "2006-01-02 15:04:05.999999"
        root.timestamp = if this.createdAt != null && this.createdAt.string().or("") != "" && this.createdAt.string() != "null" {
          this.createdAt.ts_parse("2006-01-02T15:04:05.999Z07:00").ts_format("2006-01-02 15:04:05.999999")
        } else {
          timestamp_unix().ts_format("2006-01-02 15:04:05.999999")
        }

        # ---- Build event_name SAFELY ----
        root.event_name = ""

        if this.providerName.string().or("") != "" {
          root.event_name = this.providerName.string().trim().lowercase()
        } else if this.serviceName.string().or("") != "" {
          root.event_name = this.serviceName.string().trim().lowercase()
        }

        if this.methodName.string().or("") != "" {
          if this.methodName.string() == "BEDROCK_LLM" {
            root.event_name = root.event_name + "-modeling"
          } else {
            root.event_name = root.event_name + "-" + this.methodName.string().trim().lowercase()
          }
        }

        # Only append model name when present and not JSON null (skip rather than append "null")
        let modelStr = if this.data != null && this.data.modelName != null { this.data.modelName.string().trim().lowercase() } else { "" }
        if $modelStr != "" && $modelStr != "null" {
          root.event_name = root.event_name + "-" + $modelStr
        }

        # ---- Raw payload ----
        root.payload = this.string()

        # ---- Flexible analytics fields ----
        root.field1 = this.providerName.string().or("")
        root.field2 = if this.data != null { this.data.modelName.string().or("") } else { "" }
        root.field3 = this.methodName.string().or("")
        root.field4 = this.byok.string().or("")
        root.field5 = this.dataInterface.string().or("")
        root.field6 = this.referenceType.string().or("")
        root.field7 = this.targetItemType.string().or("")
        root.field8 = this.referenceCost.string().or("")

# ============================================================================
# STEP 2: Dual Output - ClickHouse and Kafka
# ============================================================================
# - ClickHouse: Batch up to 1000 events or flush after 1 second
# - Kafka: Batch up to 100 raw events with tenant/environment 
# ============================================================================
output:
  broker:
    outputs:
      # ============================================================================
      # Output 1: ClickHouse (existing behavior)
      # ============================================================================
      - sql_insert:
          driver: clickhouse
          
          # Use native ClickHouse protocol (port 9000) - more efficient than HTTP
          # DSN is provided as a complete connection string from environment variable
          dsn: "${FLEXPRICE_CLICKHOUSE_DSN}"
          
          table: flexprice.raw_events
          
          columns:
            - id
            - tenant_id
            - environment_id
            - external_customer_id
            - event_name
            - source
            - payload
            - field1
            - field2
            - field3
            - field4
            - field5
            - field6
            - field7
            - field8
            - timestamp
          
          args_mapping: |
            root = [
              this.id,
              this.tenant_id,
              this.environment_id,
              this.external_customer_id,
              this.event_name,
              this.source,
              this.payload,
              this.field1,
              this.field2,
              this.field3,
              this.field4,
              this.field5,
              this.field6,
              this.field7,
              this.field8,
              this.timestamp
            ]
          
          batching:
            count: ${CLICKHOUSE_BATCH_COUNT:1000}        # 500–1000 batch size (use 500 if traffic is low)
            period: ${CLICKHOUSE_BATCH_PERIOD:1s}         # flush every second even if batch not full
            jitter: 0.1        # avoids thundering herd
          
          max_in_flight: ${CLICKHOUSE_MAX_IN_FLIGHT:8}     # 8 parallel async inserts to ClickHouse

      # ============================================================================
      # Output 2: Kafka raw_events topic (ENHANCED with tenant/environment)
      # ============================================================================
      # - Batch 100 raw events into {"data": [...], "tenant_id": "...", "environment_id": "..."} format
      # - Send to raw_events topic for FlexPrice consumer to process
      # - Includes tenant and environment metadata for multi-tenant support
      # ============================================================================
      - kafka:
          addresses:
            - ${FLEXPRICE_OUTPUT_KAFKA_BROKERS}

          topic: ${FLEXPRICE_OUTPUT_KAFKA_TOPIC:raw_events}

          tls:
            enabled: ${FLEXPRICE_OUTPUT_KAFKA_TLS_ENABLED:true}

          sasl:
            mechanism: ${FLEXPRICE_OUTPUT_KAFKA_SASL_MECHANISM:PLAIN}
            user: ${FLEXPRICE_OUTPUT_KAFKA_SASL_USER}
            password: ${FLEXPRICE_OUTPUT_KAFKA_SASL_PASSWORD}

          client_id: ${FLEXPRICE_OUTPUT_KAFKA_CLIENT_ID:bento-raw-events-producer}

          # Batch raw events into array of 100
          batching:
            count: ${FLEXPRICE_OUTPUT_KAFKA_BATCH_SIZE:100}
            period: ${FLEXPRICE_OUTPUT_KAFKA_BATCH_PERIOD:15s}
            processors:
              # Step 1: Restore original raw event from metadata
              - mapping: |
                  root = meta("original_raw_event").parse_json()
              # Step 2: Collect all events into a JSON array
              - archive:
                  format: json_array
              # Step 3: Wrap in {"data": [...], "tenant_id": "...", "environment_id": "..."} structure
              - mapping: |
                  root.data = this
                  root.tenant_id = meta("tenant_id").string().or("")
                  root.environment_id = meta("environment_id").string().or("")

          max_in_flight: ${FLEXPRICE_OUTPUT_KAFKA_MAX_IN_FLIGHT:10}

logger:
  level: ${LOG_LEVEL:ERROR}  # ERROR level for production (no per-message logging)
  format: logfmt
  add_timestamp: true